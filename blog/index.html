<!doctype html>
<html lang="English">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.70">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="VRapeutic Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="VRapeutic Blog Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/ed-blog/rss.xml" title="VRapeutic Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/ed-blog/atom.xml" title="VRapeutic Blog Atom Feed"><title data-react-helmet="true">AI Blog | VRapeutic</title><meta data-react-helmet="true" property="og:title" content="AI Blog | VRapeutic"><meta data-react-helmet="true" name="description" content="Blog"><meta data-react-helmet="true" property="og:description" content="Blog"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="docusaurus_locale" content="English"><meta data-react-helmet="true" name="docusaurus_tag" content="default"><link data-react-helmet="true" rel="shortcut icon" href="/img/vrapeutic-logo.ico"><link rel="stylesheet" href="/styles.ca929f20.css">
<link rel="preload" href="/styles.e1f3cb60.js" as="script">
<link rel="preload" href="/runtime~main.df94812c.js" as="script">
<link rel="preload" href="/main.cd10679b.js" as="script">
<link rel="preload" href="/1.5b5f1a29.js" as="script">
<link rel="preload" href="/2.549e171f.js" as="script">
<link rel="preload" href="/3.ac7b9500.js" as="script">
<link rel="preload" href="/a6aa9e1f.cf2fc22a.js" as="script">
<link rel="preload" href="/946ecf09.a78c87c0.js" as="script">
<link rel="preload" href="/9d1d125b.6cf08efd.js" as="script">
<link rel="preload" href="/d7084cca.d4bedcdb.js" as="script">
<link rel="preload" href="/b2b675dd.a8d4709d.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav aria-label="Skip navigation links"><button type="button" tabindex="0" class="skipToContent_11B0">Skip to main content</button></nav><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/"><img src="/img/vrapeutic-logo.ico" alt="VRapeutic Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/vrapeutic-logo.ico" alt="VRapeutic Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title">VRapeutic</strong></a><a class="navbar__item navbar__link" href="/docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog/">AI Blog</a><a class="navbar__item navbar__link" href="/ed-blog/">Yuram Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__item navbar__link">English</a><ul class="dropdown__menu"><li><a href="/blog" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active">English</a></li><li><a href="/French/blog" target="_self" rel="noopener noreferrer" class="dropdown__link">French</a></li></ul></div><a href="https://github.com/vrapeutic" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link" aria-label="GitHub repository"></a><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2N3Q"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_3NWk">🌜</span></div><div class="react-toggle-track-x"><span class="toggle_3NWk">🌞</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/"><img src="/img/vrapeutic-logo.ico" alt="VRapeutic Logo" class="themedImage_YANc themedImage--light_3CMI navbar__logo"><img src="/img/vrapeutic-logo.ico" alt="VRapeutic Logo" class="themedImage_YANc themedImage--dark_3ARp navbar__logo"><strong class="navbar__title">VRapeutic</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/">Docs</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/blog/">AI Blog</a></li><li class="menu__list-item"><a class="menu__link" href="/ed-blog/">Yuram Blog</a></li><li class="menu__list-item menu__list-item--collapsed"><a role="button" class="menu__link menu__link--sublist">Languages</a><ul class="menu__list"><li class="menu__list-item"><a href="/blog" target="_self" rel="noopener noreferrer" class="menu__link dropdown__link--active">English</a></li><li class="menu__list-item"><a href="/French/blog" target="_self" rel="noopener noreferrer" class="menu__link">French</a></li></ul></li><li class="menu__list-item"><a href="https://github.com/vrapeutic" target="_blank" rel="noopener noreferrer" class="menu__link header-github-link" aria-label="GitHub repository"></a></li></ul></div></div></div></nav><div class="main-wrapper blog-wrapper"><div class="container margin-vert--lg"><div class="row"><div class="col col--2"><div class="sidebar_SWld thin-scrollbar"><h3 class="sidebarItemTitle_Km2m">All our posts</h3><ul class="sidebarItemList_3UpA"><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/blog/blazepose-part-2">BlazePose for Full Body Keypoints Extraction.</a></li><li class="sidebarItem_2T0D"><a class="sidebarItemLink_v5H9" href="/blog/blazepose-part-1">BlazePose, the Best Model for Body keypoints Extraction [Revolutionary]</a></li></ul></div></div><main class="col col--8"><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_3-lP"><a href="/blog/blazepose-part-2">BlazePose for Full Body Keypoints Extraction.</a></h2><div class="margin-vert--md"><time datetime="2021-03-17T00:00:00.000Z" class="blogPostDate_Ta7i">March 17, 2021  · 11 min read</time></div><div class="avatar margin-vert--md"><a class="avatar__photo-link avatar__photo" href="https://www.linkedin.com/in/alaa-hesham-a6671b112/" target="_blank" rel="noreferrer noopener"><img src="https://miro.medium.com/fit/c/262/262/1*HYIF3GGs-BXlAjHXJNe-dA.png" alt="Alaa Hesham"></a><div class="avatar__intro"><h4 class="avatar__name"><a href="https://www.linkedin.com/in/alaa-hesham-a6671b112/" target="_blank" rel="noreferrer noopener">Alaa Hesham</a></h4><small class="avatar__subtitle">Machine Learning Engineer @ VRapeutic</small></div></div></header><section class="markdown"><p>A technical guide that shows the exact steps you need to do.</p><p><img src="https://miro.medium.com/max/2400/1*QveuIkCz7bYYp1_NjMicQw.jpeg" alt="MrWentworth"></p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>You can check out the <a href="https://alaa-hesham.medium.com/blazepose-for-full-body-keypoints-extraction-dc92a5bcdeb0" target="_blank" rel="noopener noreferrer">Original Medium Article</a> here.</p></div></div><p>BlazePose is a model that extracts body keypoints from a single image. It exactly infers 33, 2D landmarks of a human body from a single frame such as shoulders, elbows, and knees as illustrated in the previous figure . <strong>To know more about what it is</strong>, how its performance is revolutionary compared to its counterparts, and how to use it for upper body pose estimation , <strong>Kindly refer to <a href="https://vrapeutic.github.io/blog/blazepose-part-1" target="_blank" rel="noopener noreferrer">this article</a></strong> . It represents the first part of BlazePose’s article series.</p><p>In this article , which represents the second part of BlazePose’s article series , we will illustrate <strong>the exact technical steps you need to do to try/infer through Blazepose for full body pose estimation .</strong></p><blockquote><p>It is worth mentioning that currently, Jan 2021, the full body is only available on Android and iOS through ML Kit API.</p></blockquote><p><a href="https://developers.google.com/ml-kit" target="_blank" rel="noopener noreferrer">ML kit</a> is a mobile SDK that brings Google’s machine learning expertise to Android and iOS apps in a powerful yet easy-to-use package. It is main advantage is that you can implement the functionality you need in just a few lines of code. So where the challenge lies ?</p><p>The challenge lies in being quite newbie in mobile development such as Android Development. In this tutorial/article, we will walk you through how to try ml kit with no need to have any previous experience in Android development.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>This Article is a part of a technical research project that has been conducted by R&amp;D department at Yuram. Yuram is a product of VRapeutic– a software company specialized in therapeutic applications based in Egypt.</p></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="here-are-the-steps-you-need-to-do-summarized-we-will-then-unfold-each-step-clearly-and-in-a-great-detail"></a>Here are the steps you need to do summarized, we will then unfold each step clearly and in a great detail:<a class="hash-link" href="#here-are-the-steps-you-need-to-do-summarized-we-will-then-unfold-each-step-clearly-and-in-a-great-detail" title="Direct link to heading">#</a></h3><ul><li><p><strong>First</strong>, download and set up Integrated Development Environment (IDE) for Android app development .</p></li><li><p><strong>Second</strong>, install some components such as SDKS and Android Emulator on your IDE that you will need later on.</p></li><li><p><strong>Third</strong>, download “ML kit API” code of their quick start app in Android.</p></li><li><p><strong>Fourth</strong>, go to the file where you can run the app.</p></li><li><p><strong>Fifth</strong>, create your Android Virtual Device to run the app on . This step involves many stages where you need to : <em>Choose your device definition , determine your virtual device’s hardware capabilities such as camera, select a system image , and finally verify the configuration.</em></p></li><li><p><strong>Sixth</strong> , edit your “run/debug configuration”.</p></li><li><p><strong>Seventh</strong> , pick your Android device and hit “run”button.</p></li><li><p><strong>Eighth</strong> , dealing with ml-kit interface.</p></li><li><p><strong>Finally</strong>, showing BlazePose’s results .</p><blockquote><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="now-lets-elaborate-on-every-step"></a>Now let’s elaborate on every step<a class="hash-link" href="#now-lets-elaborate-on-every-step" title="Direct link to heading">#</a></h2></blockquote></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-first-step"></a>For the first step:<a class="hash-link" href="#for-the-first-step" title="Direct link to heading">#</a></h3><p>We need to download and set up Integrated Development Environment (IDE) for <strong>Android</strong> app development.<strong>Android Studio</strong> is the official integrated development environment (IDE) for Google’s Android operating system. I highly recommend to download and set up android studio using
<a href="https://www.youtube.com/watch?v=0zx_eFyHRU0&amp;t=3s" target="_blank" rel="noopener noreferrer">this tutorial</a>.</p><p><img alt="i5" src="/assets/images/Figure-9-8bbfc3f2e843b53671561b2c02c56c3e.jpg"></p><div align="center"><i> Android Studio Components’ installation </i></div><p>After finishing this tutorial, you now have made your “hello world” project in Android studio.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-second-step-"></a>For the second step :<a class="hash-link" href="#for-the-second-step-" title="Direct link to heading">#</a></h3><p>We are up to install some components such as SDKS and Android Emulator on your IDE that you will need later on. That is why <strong>you need to ensure that the following components are available on your Android studio</strong> by installing them if they are not installed yet.</p><ul><li>Android SDK Build-Tools</li><li>Android Emulator</li><li>Android SDK Platform-Tools</li><li>Android SDK Tools</li><li>Intel X86 Emulator Accelerator</li></ul><p>IF you are asking how to check or install them , <strong>from Tools &gt;SDK manager. &gt;Android SDK&gt;SDK Tools.</strong></p><blockquote><p>The following figures show where these orders lie in Android Studio Interface.</p></blockquote><p><img alt="i6" src="/assets/images/Figure-10-47ceb4565bf298bfb718955e2cd54eba.PNG"></p><div align="center"><i> GO to tools ,then click on SDK Manager </i></div><p><img alt="i7" src="/assets/images/Figure-12-92618c6496dfa9828f32f577dfc90ba4.png"></p><div align="center"><i>Click on Android SDK , then pick SDK Tools, select the highlighted components , finally click OK.</i></div><p><img alt="i8" src="/assets/images/Figure-11-26513ee0fadd0709a86ee50d3b534dfe.png"></p><div align="center"><i> Now switch to SDK Platforms , you may want to pick “Android 7.1.1(Nougat)” to be your SDK Platforms </i></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-third-step-"></a>For the third step :<a class="hash-link" href="#for-the-third-step-" title="Direct link to heading">#</a></h3><p>Now we are going to download “ML kit API” code of their quick start app in Android. <strong>In ML kit github repository, you can find a quick start app</strong> where you can run/infer through the model with no need to write even a single line of code. Download the code from <a href="https://github.com/googlesamples/mlkit" target="_blank" rel="noopener noreferrer">here</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-fourth-step-"></a>For the fourth step :<a class="hash-link" href="#for-the-fourth-step-" title="Direct link to heading">#</a></h3><p>Now we want to go to the file where you can run the app. <strong>Open the project using Android studio , go to the following file to run the app</strong>: mlkit-master\mlkit-master\android\vision-quickstart\app\src\main\java\com\google\mlkit\vision\demo\java\CameraXLivePreviewActivity.java</p><p>This is the activity file. An <strong>activity</strong> file represents a single screen with a user interface to interact with. If you have worked with C, C++ programming language, then this file is like your main() function.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-fifth-step-"></a>For the fifth step :<a class="hash-link" href="#for-the-fifth-step-" title="Direct link to heading">#</a></h3><blockquote><p>Now you need to create the Android virtual device (AVD) you will run your app on. Remember that ml kit is a mobile SDK. As a result, if you want to run it on your pc/laptop, you will need to install Android Emulator as we did in the second step as well as creating virtual android device with capabilities that allow you to test the app on.</p></blockquote><p>As we highlighted before this step involves many stages where you need to:</p><ol><li>Create your virtual device .</li><li>Choose your device definition .</li><li>Determine your virtual device’s hardware capabilities.</li><li>Select a system image .</li><li>Finally verify the configuration .</li></ol><blockquote><p><strong>To create your virtual device</strong> , from tools &gt;AVD manager :</p></blockquote><p><img alt="i9" src="/assets/images/Figure-13-ce4b2a37b192722e1c1fc020ffd37c1e.PNG"></p><div align="center"><i> From tools , choose AVD Manager </i></div><p>Click on “Create Virtual Device”</p><p><img alt="i10" src="/assets/images/Figure-14-b0d88ab8802d93f457ff06ae21d6cc67.JPG"></p><div align="center"><i> Create Virtual Device </i></div><blockquote><p><strong>To choose your device definition</strong>, Click on Phone tab &gt;Nesus 5&gt;click on New Hardware Profile.</p></blockquote><p><img alt="i11" src="/assets/images/Figure-15-7d71a2dfeeab9e448e35d8e5fdca21d2.jpg"></p><div align="center"><i> Device Definition </i></div><p><em>P.S. We choose Nesus 5 if you have a reason to pick another one , go on.</em></p><blockquote><p><strong>To determine your virtual device&#x27;s hardware capabilities</strong>, Click on &quot;New Hardware Profile&quot; as shown in the previous figure. Then determine the hardware capabilities of your Android Virtual Device .</p></blockquote><p>One of the options is the Camera. To enable the camera, select one or both options:</p><ul><li>Back-Facing Camera - where the lens faces away from the user.</li><li>Front-Facing Camera -where the lens faces toward the user.</li></ul><p><img alt="i12" src="/assets/images/configure-hardware-bcf67c533845aed7ebc57d814983d7e7.jpg"></p><div align="center"><i> Configure Hardware Profile </i></div><p>Now click &quot;finish&quot;, and then hit the &quot;Next&quot; button to complete your Virtual Android Device settings.</p><blockquote><p><strong>Tip</strong>: If you are willing to understand more about Android Virtual Device aka understanding why we are doing these steps , I highly recommend <a href="https://developer.android.com/studio/run/managing-avds" target="_blank" rel="noopener noreferrer">this resource .</a></p></blockquote><blockquote><p><strong>To select your System image</strong>, you will find that system images are divided into three categories : <strong>Recommended , x86 Images, and other images.</strong> However, let&#x27;s give a brief definition of system image before choosing one</p></blockquote><p>Android system images are just versions of Android that you can run on a computer. So if you wanted to test a certain version of Android, you could download and run it on the emulator to see what this Android version is like. Or if you wanted to test your app against it to check if your app runs smoothly on it.</p><p>Now the category of system image we will select is <strong>&quot;x86 image&quot;</strong>, especially we have chosen <strong>Nougat</strong> with API Level :25, ABI:x86_64 ,and Target Android 7.1.1 . It represents Android Open Source Project (AOSP) system image.</p><p><img alt="i13" src="/assets/images/Figure-16-97ef99bcb268e6bc8514386eb5bbd526.PNG"></p><div align="center"><i> x86 Images </i></div><p>We would like to justify why we have chosen this category of system images . <em>That is why the next three paragraphs will be dedicated for that purpose. You can skip them if you are not interested .</em></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="here-is-the-definition-and-usage-of-every-category"></a>Here is the definition and usage of every category:<a class="hash-link" href="#here-is-the-definition-and-usage-of-every-category" title="Direct link to heading">#</a></h3><ul><li><strong>Recommended</strong></li></ul><p>They contain access to Google Play services and they are labeled with Google APIs. The main advantage is that they provide convenient button for updating Google Play services on the device. However, this advantage comes with a price which is to ensure app security you cannot get elevated privileges root with these images. For example, you cannot directly upload files to your AVD&#x27;s external storage. That is why we did not prefer this option at the beginning.</p><ul><li><p><strong>x86 Images</strong>
This category of system images is more general. It contains images with Google Play services and others which are Android Open Source Project (AOSP) system images .If you require elevated privileges (root) to aid with your app troubleshooting, you can use AOSP system images .That is why we picked this type of system images to give us this troubleshooting freedom.</p></li><li><p><strong>Other Images</strong>
This tap includes all previous types in addition to Deprecated and out of date ones. That is why we did not prefer this option.</p></li></ul><blockquote><p><strong>To verify the configuration</strong>, <em>You need to specify the AVD Name. If you wish to use your host computer webcam or built-in camera, then choose the type of front and back camera to be Webcam0.Click on Finish</em>.</p></blockquote><p><img alt="i14" src="/assets/images/Figure-17-fe58aaa767a307543ba7ddf61ea0d377.PNG"></p><div align="center"><i> Verifying AVD Configurations </i></div><p>&quot;Congratulations for Creating your Android Virtual Device !&quot;</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-sixth-step-"></a>For the Sixth step :<a class="hash-link" href="#for-the-sixth-step-" title="Direct link to heading">#</a></h3><p>To open the Run/Debug Configurations dialog, select <strong>Run &gt; Edit Configurations</strong>. The <strong>Run/Debug Configurations</strong> dialog appears, kindly choose them to be as illustrated in the following Figure.</p><p><img alt="i15" src="/assets/images/Figure-19-965bb397647b09863dc49f96e6fff507.jpg"></p><div align="center"><i> Appropriate Run/Debug Configurations </i></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-seventh-step-"></a>For the Seventh step :<a class="hash-link" href="#for-the-seventh-step-" title="Direct link to heading">#</a></h3><p>To run the app, Pick the Name of AVD you have created. E.g. Nexus 5X API 25.</p><p><img alt="i16" src="/assets/images/Figure-20-cc782a97ca46bdb74fc20ffe4e2700f6.png"></p><div align="center"><i> Pick the Name of AVD  </i></div><p>Then, make sure that you open this file in Android studio: “mlkitmaster\mlkitmaster\android\visionquickstart\app\src\main\java\com\google\mlkit\vision\demo\java\CameraXLivePreviewActivity.java</p><p>Finally, Click the green arrow as illustrated on the following image or hit Shift+F10 buttons.</p><p><img alt="i17" src="/assets/images/Figure-21-c590fa88fc78603367c436642a3e9eeb.jpg"></p><div align="center"><i> Run the app </i></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="for-the-eighth-step"></a>For the Eighth Step:<a class="hash-link" href="#for-the-eighth-step" title="Direct link to heading">#</a></h3><blockquote><p><em>Let’s deal with ML Kit’s easy-to-use interface</em></p></blockquote><p>Now if everything is okay, you will find the following screen appear on your
AVD a. It asks you to choose which programming language you will prefer to run the app with. Personally, we have chosen Java. <strong>If running your app results in error. Kindly, refer to the next part of this article series where we talk about some technical errors that run into us and how we have solved them.</strong></p><p>Now you can choose to run the app on live camera or still images. We have chosen “StillImagesActivity” as we needed to insert static images and run the app on them.</p><p><img alt="i18" src="/assets/images/Figure-22-b7e147d145e190cb2b321e80e0537955.jpg"></p><dev align="center"><i>Left Figure: ML Kit interface.</i></dev><dev align="center"><i> Right Figure : Infer the model on static images, choose the second option.</i></dev><p>Since we want to try BlazePose for pose detection, click on the arrow as illustrated on the following image and choosing <strong>Pose Detection</strong>. It will ask you if you want to <strong>choose images from album</strong> or in other words from your Android Virtual Device storage <strong>or take image</strong> using your host computer’s webcam. As we have already uploaded images to test on AVD’s storage, we have chosen to select image from album.</p><p><img alt="i19" src="/assets/images/Figure-23-3a6949acb3bbb1442875cd03f93d73df.jpg"></p><dev align="center"><i>Left Figure : Pose Detection</i></dev><dev align="center"><i>Right Figure : Choose Select image from album</i></dev><p>Now you can choose images from your AVD internal storage or external
storage.</p><p>If you do not know how to upload images to your AVD’s storage nor how to view them. <strong>Kindly follow us to get notified when the third part of this article series is available where we explain how to upload images and overcome the problem of not being able to view them on AVD.</strong></p><p>Once you have chosen the image , you will find that the model has been applied on it and you will see body keypoints illustrated in white as in the following image.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="congratulationsyou-have-earned-it--here-is-the-final-step-where-we-view-the-results"></a><strong>Congratulations…..You have Earned it .. Here is the Final step where we view the Results</strong><a class="hash-link" href="#congratulationsyou-have-earned-it--here-is-the-final-step-where-we-view-the-results" title="Direct link to heading">#</a></h3><p><img alt="i20" src="/assets/images/Figure-24-44fee61ddf71499be1d0207ca8444506.jpg"></p><dev align="center"><i> Apply the model on the image. </i></dev><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="blazeposes-results"></a>BlazePose&#x27;s Results:<a class="hash-link" href="#blazeposes-results" title="Direct link to heading">#</a></h3><p>Now let’s view more examples of Blazepose’s performance for full body keypoints Extraction.</p><p><img alt="i21" src="/assets/images/Figure-25-2f7017d18e7a09356124381fe1066cbb.jpg"></p><dev align="center"><i> BlazePose’s Results </i></dev><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="conclusion"></a>Conclusion:<a class="hash-link" href="#conclusion" title="Direct link to heading">#</a></h3><p><strong>In this part</strong> , we have illustrated the exact steps you need to do to try/infer through Blazepose for full body pose estimation. <strong>In the next part</strong> , we will view some of the errors which there is a chance to encounter and how to solve them. So you may want to follow us to get notified when the next part is out.</p></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/computer-vision">computer-vision</a><a class="margin-horiz--sm" href="/blog/tags/research">research</a><a class="margin-horiz--sm" href="/blog/tags/pose-estimation">pose-estimation</a><a class="margin-horiz--sm" href="/blog/tags/virtual-reality">virtual reality</a></div></footer></article><article class="margin-bottom--xl"><header><h2 class="margin-bottom--sm blogPostTitle_3-lP"><a href="/blog/blazepose-part-1">BlazePose, the Best Model for Body keypoints Extraction [Revolutionary]</a></h2><div class="margin-vert--md"><time datetime="2021-01-17T00:00:00.000Z" class="blogPostDate_Ta7i">January 17, 2021  · 5 min read</time></div><div class="avatar margin-vert--md"><a class="avatar__photo-link avatar__photo" href="https://www.linkedin.com/in/alaa-hesham-a6671b112/" target="_blank" rel="noreferrer noopener"><img src="https://miro.medium.com/fit/c/262/262/1*HYIF3GGs-BXlAjHXJNe-dA.png" alt="Alaa Hesham"></a><div class="avatar__intro"><h4 class="avatar__name"><a href="https://www.linkedin.com/in/alaa-hesham-a6671b112/" target="_blank" rel="noreferrer noopener">Alaa Hesham</a></h4><small class="avatar__subtitle">Machine Learning Research @ VRapeutic</small></div></div></header><section class="markdown"><p>A comprehensive technical guide on how to extract human body keypoints using BlazePose for Human Pose Estimation.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>You can check out the <a href="https://alaa-hesham.medium.com/blazepose-the-best-model-for-body-keypoints-extraction-efdad71d74e2" target="_blank" rel="noopener noreferrer">Original Medium Article</a> here.</p></div></div><p><img src="https://miro.medium.com/max/2400/1*KRzZd1ofbP-FMWj1MPrvhQ.jpeg" alt="key-points"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="what-is-blazepose"></a>What is BlazePose?<a class="hash-link" href="#what-is-blazepose" title="Direct link to heading">#</a></h2><p>BlazePose is a model that extracts body keypoints from a single image. It exactly infers 33, 2D landmarks of a human body from a single frame such as shoulders, elbows, and knees as illustrated in the following figure. The user’s face must be in the image to detect the pose. To have the best results, the person’s entire body should be in the image. Google researchers have provided this human pose perception functionality to the broader research and development community to encourage the emergence of creative use cases, and stimulate new applications and research avenues.</p><blockquote><p>Its edge is that it is more suitable for applications like fitness ,rehabilitation and dance than already existing models. Why? It is more accurate. It also localizes more keypoints than other previous models do to be more suitable for applications where the scale and orientation of hands and feet are vital information.</p></blockquote><p>For more theoretical information about BlazePose model, check <a href="https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html" target="_blank" rel="noopener noreferrer">Google AI Blog here</a>.</p><p><img src="https://miro.medium.com/max/2400/1*-5YeZDrwsfHCGmOj8NFOsg.jpeg" alt="key-points-2">
<em>Body keypoints that BlazePose detects</em></p><p><em>In this part of <strong>Blazepose’s article series</strong>, we will cover the following points:</em></p><ul><li>How BlazePose’s performance exceeds already existing models such as PoseNet ?</li><li>How to easily try/infer through Blazepose for upper body pose estimation ?</li></ul><p><em>In the <strong>second part</strong>, we will explain:</em></p><ul><li>The exact steps you need to do to try/infer through Blazepose for full body pose estimation. It is worth mentioning that currently the full body is only available on Android and iOS through ML Kit API.</li><li>ML kit interface and how to use it ?</li></ul><p><em>In the <strong>third part</strong>, we will discuss:</em></p><ul><li>Some of technical problems and how to troubleshoot/solve them ?</li></ul><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>This Article is a part of a technical research project that has been conducted by R&amp;D department at Yuram. Yuram is a product of VRapeutic– a software company specialized in therapeutic applications based in Egypt.</p></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-its-performance-exceeds-already-existing-models-such-as-posenet"></a>How its performance exceeds already existing models such as PoseNet?<a class="hash-link" href="#how-its-performance-exceeds-already-existing-models-such-as-posenet" title="Direct link to heading">#</a></h2><p>Before trying BlazePose for pose estimation, <strong>we have tried other models such as PoseNet</strong>. According to literature review, PoseNet is one of the best models that existed before the appearance of BlazePose. So let’s see how BlazePose enhanced the performance of some cases where PoseNet does not manage to achieve good nor reliable results.</p><p>We have noticed that PoseNet tends to have a good performance when the picture is zoomed out as in Figure 3 . However, it performs quite poorly when the subject person in the image is near as in Figure 2 and 4 even if the image is of good quality and resolution. It also does not manage to produce some body keypoints when the subjects’ clothes are fluffy such as in Figure 5.</p><p><img alt="i1" src="/assets/images/key-pts-1-47255476ea37047f1c5e02152d83d4dc.png">
<em>(Left) PoseNet could not extract keypoints when image size is that big. (Right) PoseNet managed to extract keypoints when image becomes of lower
size and zoomed out.</em></p><p><img alt="i2" src="/assets/images/key-pts-2-dad40e4c73f82e28f877b5f2ec90ecab.png">
<em>(Left) PoseNet does not manage to extract all body keypoints even that the image is
of good quality. (Right) When person’s clothes are fluffy, it did not manage to detect all body
key points.</em></p><p><strong>Now let’s see how BlazePose performs on these
images.</strong></p><p><img alt="i3" src="/assets/images/key-pts-3-a919fbbc6379897d99679e3bd54c479b.png"></p><p>Apparently, its performance is superior comparing to its current counterpart. <strong>Disclaimer</strong>, keypoints appear in white color but we changed their color to green. The reason is to make them clearer definitely with the same exact position and orientation.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-to-try-blazepose-"></a>How to try BlazePose ?<a class="hash-link" href="#how-to-try-blazepose-" title="Direct link to heading">#</a></h2><p>Although pose estimation for <strong>upper body</strong> is supported on many platforms such as Android, iOS, Desktop, Python, and Web. <strong>The full body is supported only on Android and iOS. Other platforms support upper body only</strong>. (This is the current situation in Jan 2021)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="how-to-easily-tryinfer-through-blazepose-for-upper-body-pose-estimation-"></a>How to easily try/infer through Blazepose for upper body pose estimation ?<a class="hash-link" href="#how-to-easily-tryinfer-through-blazepose-for-upper-body-pose-estimation-" title="Direct link to heading">#</a></h2><p>To try BlazePose performance on the upper body, the easiest way is
through Python notebook developed by MediaPipe. <em>Mediapipe is an open source framework to build word-class machine learning solutions by Google</em>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="in-simple-terms-do-the-following-steps"></a>In simple terms, do the following steps:<a class="hash-link" href="#in-simple-terms-do-the-following-steps" title="Direct link to heading">#</a></h3><ol><li><p>Go to the following <a href="https://colab.research.google.com/drive/1uCuA6We9T5r0WljspEHWPHXCT_2bMKUy" target="_blank" rel="noopener noreferrer">Colab notebook</a></p></li><li><p>From File menu, choose Save a copy In Drive</p></li></ol><p><img src="https://miro.medium.com/max/700/1*05wWwL3XCdcMtRKmefzB3g.png" alt="i8"></p><ol start="3"><li>Go to the copied notebook, run the first cell. In the second cell, run it and
click on choose Files button to choose the image you want to try the model
on.</li></ol><p><img src="https://miro.medium.com/max/430/1*Cynj7U08HJOQcSnkTPFKHw.jpeg" alt="i9"></p><ol start="4"><li>Keep running the following cells, finally you will get the input image
annotated with upper body keypoints.</li></ol><p><img alt="i4" src="/assets/images/key-pts-4-7b94afd290be13aca69c6a7342fa96f2.png"></p><p><em>The left image represents the input image while the right image represents the annotated output image.</em></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_prK2" id="conclusion"></a>Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">#</a></h2><p>In this Article, we have just illustrated what is BlazePose, how its performance is revolutionary compared to its counterparts, and how to use BlazePose for upper body pose estimation. <strong>For Full Body Pose Estimation/Keypoints Extraction, kindly follow us to get notified when the second part is available where we explain the exact technical steps you need to do.</strong></p></section><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/computer-vision">computer-vision</a><a class="margin-horiz--sm" href="/blog/tags/research">research</a><a class="margin-horiz--sm" href="/blog/tags/pose-estimation">pose-estimation</a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Docs</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs/">WebXR</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/ellie-tale/">Therapeutic Modules&#x27; Library</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Blogs</h4><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/blog/">AI Blog</a></li><li class="footer__item"><a class="footer__link-item" href="/ed-blog">Yuram Blog</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">VRapeutic on Social Media</h4><ul class="footer__items"><li class="footer__item"><a href="https://www.facebook.com/myvrapeutic" target="_blank" rel="noopener noreferrer" class="footer__link-item">Facebook</a></li><li class="footer__item"><a href="https://twitter.com/myvrapeutic" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li><li class="footer__item"><a href="https://www.instagram.com/myvrapeutic/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Yuram on Social Media</h4><ul class="footer__items"><li class="footer__item"><a href="https://www.facebook.com/yuramcares/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Facebook</a></li><li class="footer__item"><a href="https://twitter.com/yuramcares" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li><li class="footer__item"><a href="https://www.instagram.com/yuramcares/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Instagram</a></li></ul></div></div><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a href="https://myvrapeutic.com/" target="_blank" rel="noopener noreferrer" class="footerLogoLink_31Aa"><img class="footer__logo" alt="VRapeutic Logo" src="/img/vrapeutic-logo.ico"></a></div><div class="footer__copyright">Copyright © 2021 VRapeutic, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/styles.e1f3cb60.js"></script>
<script src="/runtime~main.df94812c.js"></script>
<script src="/main.cd10679b.js"></script>
<script src="/1.5b5f1a29.js"></script>
<script src="/2.549e171f.js"></script>
<script src="/3.ac7b9500.js"></script>
<script src="/a6aa9e1f.cf2fc22a.js"></script>
<script src="/946ecf09.a78c87c0.js"></script>
<script src="/9d1d125b.6cf08efd.js"></script>
<script src="/d7084cca.d4bedcdb.js"></script>
<script src="/b2b675dd.a8d4709d.js"></script>
</body>
</html>